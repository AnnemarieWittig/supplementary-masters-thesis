= The Productivity Impact of Software Developers Using LLM-based Coding Assistants: Analysis Scripts

This directory contains the statistical analysis scripts used to process repository data and generate productivity metrics for comparing development patterns before and after the introduction of LLM-based coding assistants.

== Prerequisites

=== Python Environment
The scripts require Python with the following dependencies:

* pandas
* numpy
* matplotlib
* scipy
* python-dotenv
* jupyter (for notebook scripts)

Install dependencies via pip:
[source,bash]
----
pip install pandas numpy matplotlib scipy python-dotenv jupyter
----

=== Helper Modules
The analysis scripts depend on custom helper modules:

* `helper.general` - Contains functions for data processing, date splitting, bucket generation, and path handling
* `helper.significance` - Contains statistical significance testing functions including normality checks and effect size calculations

=== Data Structure
The scripts expect data in CSV format with the following files per repository/person:

* `commits.csv` - Commit data with columns: sha, author, date, message, loc_added, loc_deleted
* `pull_requests.csv` - Pull request data with columns: author, merged_by, title, description, etc.
* `releases.csv` - Release data with columns: author, message, date
* `branches.csv` - Branch data with columns: created_by, last_author
* `files.json` - File change data with commit_sha references; is used to create `commits_file_level_changes.csv`
* `commits_file_level_changes.csv` - File-level commit data with detailed metrics per file per commit, including lines added/removed, calculated changes, and relative churn metrics (generated by Transform_json.ipynb)

== Configuration

=== Environment Variables (.env file)
To run the analysis scripts, you require a `.env` file in the same directory. The structure and preparation of the data depends on whether an analysis per repository or per person is intended.

==== Per Repository Analysis
Example configuration:
[source,env]
----
STORAGE_DIRECTORIES=['/path/to/data/repositories']
RESULTS_DIRECTORY='/path/to/results/output'
INTRO_DATE=2022-06-21
BUCKET_SIZE=90
START_DATE=2019-10-05
END_DATE=2025-03-07
PARTICIPANTS_TO_REMOVE=['repository1','repository2','repository3']
MAPPING_PATH=/path/to/data/repositories/mapping.json
PER_PERSON=False
----

==== Per Person Analysis
Example configuration:
[source,env]
----
STORAGE_DIRECTORIES=['/path/to/data/by_person']
RESULTS_DIRECTORY='/path/to/results/output'
INTRO_DATE=
BUCKET_SIZE=14
START_DATE=
END_DATE=
PARTICIPANTS_TO_REMOVE=['participant1','participant2','participant3']
MAPPING_PATH=/path/to/data/by_person/mapping.json
PER_PERSON=True
----

==== Configuration Parameters
* `STORAGE_DIRECTORIES`: List of directories containing the repository/person data
* `RESULTS_DIRECTORY`: Output directory for generated analysis results
* `INTRO_DATE`: Date when the LLM-based coding assistant was introduced (YYYY-MM-DD format)
* `BUCKET_SIZE`: Number of days to group data into buckets for analysis
* `START_DATE`: Analysis start date (optional, overrides mapping.json)
* `END_DATE`: Analysis end date (optional, overrides mapping.json)
* `MAPPING_PATH`: Path to mapping.json file for individual configurations
* `PER_PERSON`: Boolean flag for person-level vs repository-level analysis
* `PARTICIPANTS_TO_REMOVE`: List of participant (or repository) IDs to exclude from analysis

=== Mapping Configuration (mapping.json)

Some scripts expect a `mapping.json` file in the root of the folder provided in the first storage directory in the `STORAGE_DIRECTORIES` list. This file allows for individual configuration of each repository or person.

[source,json]
----
{
    "repository_name_or_person_id": {
        "SYNONYM": "Display name for reports",
        "END_DATE": "2025-01-27",
        "INTRO_DATE": "2024-09-23", 
        "START_DATE": "2024-05-20"
    }
}
----

*Important Notes:*
* Date fields should only be added if there are individual different dates for each repository/person
* The key must align with the corresponding folder name in `STORAGE_DIRECTORIES`
* Dates must be in YYYY-MM-DD format
* If `INTRO_DATE` is empty in .env, values from mapping.json will be used

== Available Analysis Scripts

=== Data Preprocessing

* **link:separate_into_participants.py[`separate_into_participants.py`]** - Restructures repository data into person-level datasets by extracting and filtering data based on user participation across multiple repositories
* **link:Transform_json.ipynb[`Transform_json.ipynb`]** - Transforms file level commit information (`files.json`) into `commits_file_level_changes.csv` with detailed per-file metrics and relative churn calculations

=== Commit-Based Metrics

* **link:commit_total.ipynb[`commit_total.ipynb`]** - Analyzes total number of commits over time periods
* **link:commit_LOC_added.ipynb[`commit_LOC_added.ipynb`]** - Analyzes lines of code added per commit
* **link:commit_LOC_deleted.ipynb[`commit_LOC_deleted.ipynb`]** - Analyzes lines of code deleted per commit  
* **link:commit_LOC_total_changed.ipynb[`commit_LOC_total_changed.ipynb`]** - Analyzes total lines of code changed (added + deleted)
* **link:commit_relative_churn_M1.ipynb[`commit_relative_churn_M1.ipynb`]** - Calculates code churn M1; requires `Transform_json.ipynb` to run first to transform the file level jsons to csv
* **link:commit_relative_churn_M2.ipynb[`commit_relative_churn_M2.ipynb`]** - Calculates code churn M2; requires `Transform_json.ipynb` to run first to transform the file level jsons to csv
* **link:commit_relative_churn_M7.ipynb[`commit_relative_churn_M7.ipynb`]** - Calculates code churn M7; requires `Transform_json.ipynb` to run first to transform the file level jsons to csv
* **link:coupling.ipynb[`coupling.ipynb`]** - Analyzes file coupling metrics based on commit patterns

=== Pull Request Metrics  
* **link:pull_requests_total.ipynb[`pull_requests_total.ipynb`]** - Analyzes total number of pull requests 
* **link:pull_requests_successful.ipynb[`pull_requests_successful.ipynb`]** - Analyzes successful pull request completion rates
* **link:pull_requests_time_to_close.ipynb[`pull_requests_time_to_close.ipynb`]** - Analyzes time from PR creation to closure
* **link:pull_requests_time_to_merge.ipynb[`pull_requests_time_to_merge.ipynb`]** - Analyzes time from PR creation to merge

=== Other Metrics

* **link:release_total.ipynb[`release_total.ipynb`]** - Analyzes total number of releases over time
* **link:time_until_merged.ipynb[`time_until_merged.ipynb`]** - Analyzes the time until a commit

== Running the Analysis

=== Step 1: Data Preparation
1. Ensure your data follows the expected CSV structure
2. If analyzing by person, run `separate_into_participants.py` first to restructure the data
3. Create the mapping.json file if using individual configurations per repository/person

=== Step 2: Environment Setup
1. Create a `.env` file with the appropriate configuration
2. Create a `mapping.json` if wanted
3. Install all required dependencies

=== Step 3: Execute Analysis
Run the Jupyter notebooks in any order based on the metrics you want to analyze:

[source,bash]
----
jupyter notebook commit_total.ipynb
----

Or execute all notebooks programmatically:
[source,bash]
----
python -m jupyter nbconvert --execute --to notebook *.ipynb
----

== Output

All analysis scripts generate:

* **CSV files** with calculated metrics stored in `RESULTS_DIRECTORY/metric_calculation_{BUCKET_SIZE}/`
* **Statistical significance tests** comparing pre- and post-introduction periods; included for completeness, not part of analysis
* **Effect size calculations** using Cliff's Delta
* **Visualizations** showing metric trends over time

The results are bucketed into time periods of `BUCKET_SIZE` days, with separate buckets for pre-introduction and post-introduction periods.
